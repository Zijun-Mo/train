# 面部表情动态评估训练系统

## 项目概述

本项目旨在开发一个深度学习模型，用于评估面部表情的动态性和联动性。系统结合了光流图像分析和面部关键点特征，输出两个评分：动态评分（0-5）和联动评分（0-3）。

## 数据集结构

### 数据组织方式
```
output/
├── eye_close/
│   ├── 01_001/
│   │   ├── optical_flow.jpg        # 光流图像 (3×112×112)
│   │   ├── expression_landmarks.npy # 表情关键点 (146×2)
│   │   ├── baseline_landmarks.npy   # 基线关键点 (146×2)
│   │   └── expression_g_column.json # 标注信息
│   └── ...
└── ...
```

### 数据说明
- **视频编号**: 01, 02, 03... (用于训练/验证集划分)
- **帧编号**: 001, 002, 003... (视频中的帧序号)
- **输入数据**:
  - `optical_flow.jpg`: 光流图像，尺寸为3×112×112
  - `expression_landmarks.npy`: 当前表情的146个面部关键点坐标
  - `baseline_landmarks.npy`: 基线表情的146个面部关键点坐标
- **输出标签**: 
  - `dynamics`: 动态评分 (0-5整数)
  - `synkinesis`: 联动评分 (0-3整数)

## 模型架构

### 1. 预训练模型
- **面部关键点模型**: `face_blendshapes.tflite`
  - 输入: 146×2的关键点坐标
  - 输出: 52维特征向量
  - 模型大小: 0.91 MB

### 2. 整体架构
```
输入层:
├── 光流图像 (3×112×112) → ResNet-18预训练模型 → 光流特征
├── 表情关键点 (146×2) → face_blendshapes模型 → 52维特征
└── 基线关键点 (146×2) → face_blendshapes模型 → 52维特征

特征融合:
├── 关键点特征 (52+52=104维) → 全连接层 → 2维中间特征
└── 光流特征 → 2维中间特征

最终输出:
└── 融合特征 (2+2=4维) → 全连接层 → 最终评分 (2维)
```

### 3. 模型组件
1. **光流解析模型**: 基于ResNet-18的光流特征提取器
2. **特征点解析模型**: 104维到2维的全连接网络
3. **融合模型**: 4维到2维的最终评分网络

## 训练策略

### 阶段一: 光流模型预训练
1. 使用ResNet-18预训练权重
2. 微调网络输出层，使其直接预测2维评分向量
3. 冻结其他组件，只训练光流路径

### 阶段二a: 关键点全连接层训练
1. 冻结两个face_blendshapes模型
2. 使用较高学习率（0.01）训练104维到2维的全连接层
3. 使用关键点特征直接预测评分

### 阶段二b: 关键点模型微调
1. 解冻face_blendshapes模型进行端到端微调
2. 使用较低学习率（0.001）进行精细调优
3. 优化整个关键点处理路径

### 阶段三: 融合模型训练
1. 冻结前两个阶段的模型
2. 训练4维到2维的融合网络
3. 结合光流和关键点的中间特征

### 阶段四: 端到端微调
1. 解冻所有可训练参数
2. 进行端到端的细粒度调优
3. 使用较小的学习率进行训练

## 数据划分策略

为防止数据泄露，按视频编号进行训练/验证集划分：
- 训练集: 80%的视频
- 验证集: 20%的视频
- 同一视频的所有帧都在同一个集合中

## 配置文件

使用YAML配置文件管理训练参数，包括：
- 数据路径配置
- 模型超参数
- 训练策略参数
- 数据增强设置

## 文件结构

```
train/
├── README.md              # 项目说明文档
├── config.yaml           # 配置文件
├── main.py               # 主训练脚本
├── models/
│   ├── __init__.py
│   ├── optical_flow_model.py    # 光流模型定义
│   ├── landmark_model.py        # 关键点模型定义
│   ├── fusion_model.py          # 融合模型定义
│   └── face_blendshapes.py      # TFLite模型包装器
├── data/
│   ├── __init__.py
│   ├── dataset.py              # 数据集类定义
│   └── transforms.py           # 数据预处理
├── training/
│   ├── __init__.py
│   ├── trainer.py              # 训练器类
│   └── utils.py                # 训练工具函数
└── utils/
    ├── __init__.py
    ├── config.py               # 配置文件解析
    └── metrics.py              # 评估指标
```

## 环境要求

- Python 3.8+
- PyTorch 1.9+
- TensorFlow Lite 2.x
- OpenCV
- NumPy
- PyYAML
- torchvision
- matplotlib (可选，用于可视化)

## 使用方法

1. 配置`config.yaml`文件中的参数
2. 运行主训练脚本: `python main.py`
3. 模型将按照五个阶段依次训练
4. 训练过程和结果将保存在指定目录中

## 评估指标

- **主要指标**: 
  - 动态评分的均方误差 (MSE)
  - 联动评分的均方误差 (MSE)
  - 总体预测准确度
- **辅助指标**:
  - 各评分等级的分类准确率
  - 混淆矩阵分析

## 训练结果可视化

训练完成后，系统会自动生成以下可视化文件：

### 1. 综合训练曲线图 (`training_curves.png`)
- 所有阶段的训练和验证损失曲线
- 准确率变化趋势
- 各阶段性能对比

### 2. 分阶段损失图 (`stage_losses.png`)
- **阶段1**: 光流模型预训练损失曲线
- **阶段2a**: 关键点全连接层训练损失曲线
- **阶段2b**: 关键点模型微调损失曲线  
- **阶段3**: 融合模型训练损失曲线
- **阶段4**: 端到端微调损失曲线
- 每个阶段的训练/验证损失对比

### 3. 训练历史数据 (`training_history.json`)
- 详细的训练指标记录
- 每个epoch的损失和准确率数据
- 便于后续分析和复现
